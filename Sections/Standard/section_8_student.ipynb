{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A/STAT 121A/AC 209A/CSCI E-109A\n",
    "\n",
    "## Standard Section 8: Multiclass Model and Midterm Review\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2017**<br/>\n",
    "**Section Leaders: Albert Wu, Nathaniel Burbank<br/>**\n",
    "**Instructors: Pavlos Protopapas, Kevin Rader, Rahul Dave, Margo Levine** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>**Download this notebook from the CS109 repo or here:**</center>\n",
    "<center>**http://bit.ly/109_S8**</center>\n",
    "\n",
    "This section can be split into two major parts. The first covers multiclass model fitting and is designed to help you get started on HW 7. The second part goes over midterm 2 from 2016 and is designed to help you prepare for the upcoming midterm. \n",
    "\n",
    "Specifically, we will: \n",
    "    \n",
    "    1. Use the iris dataset, which we used in section 3, to cover multiclass models and fitting them.\n",
    "    2. Dive into midterm 2 from last year and go through a thorough review of the solutions that were posted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.tree import DecisionTreeClassifier as DecisionTree\n",
    "from sklearn.ensemble import RandomForestClassifier as RandomForest\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.display.max_columns = 500\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#--------  plot_decision_boundary\n",
    "# A function that visualizes the data and the decision boundaries\n",
    "# Input: \n",
    "#      x (predictors)\n",
    "#      y (labels)\n",
    "#      model (the classifier you want to visualize)\n",
    "#      title (title for plot)\n",
    "#      ax (a set of axes to plot on)\n",
    "#      poly_degree (highest degree of polynomial terms included in the model; None by default)\n",
    "\n",
    "def plot_decision_boundary(x, y, model, title, ax, poly_degree=None):\n",
    "    \n",
    "    # Create mesh\n",
    "    offset = .1\n",
    "    # Interval of points for Sepal length\n",
    "    min0 = x[:,0].min() - offset\n",
    "    max0 = x[:,0].max() + offset\n",
    "    interval0 = np.arange(min0, max0, (max0-min0)/300)\n",
    "    n0 = np.size(interval0)\n",
    "    \n",
    "    # Interval of points for Petal width\n",
    "    min1 = x[:,1].min() - offset\n",
    "    max1 = x[:,1].max() + offset\n",
    "    interval1 = np.arange(min1, max1, (max1-min1)/300)\n",
    "    n1 = np.size(interval1)\n",
    "\n",
    "    # Create mesh grid of points\n",
    "    x1, x2 = np.meshgrid(interval0, interval1)\n",
    "    x1 = x1.reshape(-1,1)\n",
    "    x2 = x2.reshape(-1,1)\n",
    "    xx = np.concatenate((x1, x2), axis=1)\n",
    "\n",
    "    # Predict on mesh of points\n",
    "    # Check if polynomial terms need to be included\n",
    "    if(poly_degree!=None):\n",
    "        # Use PolynomialFeatures to generate polynomial terms\n",
    "        poly = PolynomialFeatures(poly_degree,include_bias = False)\n",
    "        xx_ = poly.fit_transform(xx)\n",
    "        yy = model.predict(xx_) \n",
    "        \n",
    "    else:   \n",
    "        yy = model.predict(xx)\n",
    "        \n",
    "    yy = yy.reshape((n0, n1))\n",
    "\n",
    "    # Plot decision surface\n",
    "    x1 = x1.reshape(n0, n1)\n",
    "    x2 = x2.reshape(n0, n1)\n",
    "    ax.contourf(x1, x2, yy, cmap=plt.cm.coolwarm, alpha=0.1)\n",
    "    \n",
    "    # Plot scatter plot of data\n",
    "    yy = y.reshape(-1,)\n",
    "    ax.scatter(x[yy==0,0], x[yy==0,1], label='Class 0', cmap=plt.cm.bwr, edgecolor='k')\n",
    "    \n",
    "    ax.scatter(x[yy==1,0], x[yy==1,1], label='Class 1',cmap=plt.cm.bwr, edgecolor='k')\n",
    "    \n",
    "    ax.scatter(x[yy==2,0], x[yy==2,1], label='Class 2', cmap=plt.cm.bwr, edgecolor='k')\n",
    "    \n",
    "    # Label axis, title\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Sepal length')\n",
    "    ax.set_ylabel('Petal width')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Fitting Multiclass Models to the Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first part, part 0, is designed to help you get started on the first part of the homework 7. The dataset we use is similar to that of section 3, where we had flower type as our predictor (0,1,2). Here we will focus on sepal length and petal width as our predictors. We will fit and compare the training and test accuracies of the following classification methods:\n",
    "\n",
    "- Multiclass Logistic Regression (Multinomial and one-vs-rest (OvR))\n",
    "- Multiclass Logistic Regression cubic terms\n",
    "- Linear Discriminant Analysis\n",
    "- Quadratic Discriminant Analysis\n",
    "- k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/cs109/a-2017/master/Sections/Standard/s8_data/section_8_data.csv')\n",
    "np.random.seed(seed=109)\n",
    "msk = np.random.rand(len(df)) < 0.5\n",
    "data_train_2 = df[msk]\n",
    "data_test_2 = df[~msk]\n",
    "data_train_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column `target` contains our three types of flowers (0,1,2), while `slength` and `pwidth` are the sepal legnth and petal width for each specific flower observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the training points\n",
    "plt.scatter(data_train_2.iloc[:, 0], data_train_2.iloc[:, 1], c=data_train_2.iloc[:,2].values, cmap=plt.cm.bwr,\n",
    "            edgecolor='k')\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Petal width')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that for our case, a linear classifier will not be able to perfectly separate the points. But it does appear that a linear classifier should seem to do relatively well here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use multinomial logistic regression and one-vs-rest (OvR) logistic regression methods for fitting a multiclass classifier. In OvR, a separate binary classifier is fit for each class, whereas in multinomial logistic regression a single classifier is fit for all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_2 = data_train_2[['slength', 'pwidth']]\n",
    "y_train_2 = data_train_2['target']\n",
    "X_test_2 = data_test_2[['slength', 'pwidth']]\n",
    "y_test_2 = data_test_2['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will fit both a OvR logistic regression and Multinomial logistic regression model. First, we look at the implied decision boundaries from both models on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logregcv = LogisticRegressionCV(multi_class='ovr')\n",
    "logregcv.fit(X_train_2, y_train_2)\n",
    "logregcv_2 = LogisticRegressionCV(multi_class='multinomial')\n",
    "logregcv_2.fit(X_train_2, y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(2, 2, figsize = (13,13))\n",
    "\n",
    "plot_decision_boundary(X_train_2.values, y_train_2.values, \n",
    "                               logregcv, \"LogReg OvR - Train\", axes[0,0])\n",
    "\n",
    "plot_decision_boundary(X_train_2.values, y_train_2.values, \n",
    "                               logregcv_2, \"LogReg Multinomial - Train\", axes[0,1])\n",
    "\n",
    "plot_decision_boundary(X_test_2.values, y_test_2.values, \n",
    "                               logregcv, \"LogReg OvR - Test\", axes[1,0])\n",
    "\n",
    "plot_decision_boundary(X_test_2.values, y_test_2.values, \n",
    "                               logregcv_2, \"LogReg Multinomial - Test\", axes[1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we look we look at the training and testing accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Train OvR:\", logregcv.score(X_train_2, y_train_2))\n",
    "print(\"Test OvR:\", logregcv.score(X_test_2, y_test_2))\n",
    "print(\"Train Multinomial:\", logregcv_2.score(X_train_2, y_train_2))\n",
    "print(\"Test Multinomial:\",logregcv_2.score(X_test_2, y_test_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we show how to fit a Multiclass Logistic Regression model with cubic terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poly_degree = 3\n",
    "poly = PolynomialFeatures(poly_degree, include_bias=False)\n",
    "X_train_poly_cubic = poly.fit_transform(X_train_2)\n",
    "X_test_poly_cubic = poly.fit_transform(X_test_2)\n",
    "\n",
    "logregcv_cubic = LogisticRegressionCV()\n",
    "logregcv_cubic.fit(X_train_poly_cubic, y_train_2)\n",
    "\n",
    "logregcv_2_poly_cubic = LogisticRegressionCV(multi_class='multinomial')\n",
    "logregcv_2_poly_cubic.fit(X_train_poly_cubic, y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 2, figsize = (15,8))\n",
    "\n",
    "plot_decision_boundary(X_train_poly_cubic, y_train_2.values, \n",
    "                               logregcv_cubic, \"LogReg OvR Cubic\", axes[0],poly_degree)\n",
    "\n",
    "plot_decision_boundary(X_train_poly_cubic, y_train_2.values, \n",
    "                               logregcv_2_poly_cubic, \"LogReg Multinomial Cubic\", axes[1],poly_degree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Train LR Cubic Features OvR:\", logregcv_cubic.score(X_train_poly_cubic, y_train_2))\n",
    "print(\"Test LR Cubic Features OvR:\", logregcv_cubic.score(X_test_poly_cubic, y_test_2))\n",
    "print(\"Train LR Cubic Features Multinomial:\", logregcv_2_poly_cubic.score(X_train_poly_cubic, y_train_2))\n",
    "print(\"Test LR Cubic Features Multinomial:\",logregcv_2_poly_cubic.score(X_test_poly_cubic, y_test_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then compare our results to that given by LDA and QDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda = LDA()\n",
    "lda.fit(X_train_2, y_train_2)\n",
    "qda = QDA()\n",
    "qda.fit(X_train_2, y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(2, 2, figsize = (12,12))\n",
    "\n",
    "plot_decision_boundary(X_train_2.values, y_train_2.values, \n",
    "                               lda, \"Linear Discriminant Analysis (LDA) - Train\", axes[0,0])\n",
    "\n",
    "plot_decision_boundary(X_train_2.values, y_train_2.values, \n",
    "                               qda, \"Quadratic Discriminant Analysis (QDA) - Train\", axes[0,1])\n",
    "\n",
    "plot_decision_boundary(X_test_2.values, y_test_2.values, \n",
    "                               lda, \"Linear Discriminant Analysis (LDA) - Test\", axes[1,0])\n",
    "\n",
    "plot_decision_boundary(X_test_2.values, y_test_2.values, \n",
    "                               qda, \"Quadratic Discriminant Analysis (QDA) - Test\", axes[1,1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Train LDA:\", lda.score(X_train_2, y_train_2))\n",
    "print(\"Test LDA:\", lda.score(X_test_2, y_test_2))\n",
    "print(\"Train QDA:\", qda.score(X_train_2, y_train_2))\n",
    "print(\"Test QDA:\",qda.score(X_test_2, y_test_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we cover the fitting of k-Nearest Neighbors for $k = 2,5$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn_2 = KNN(n_neighbors=2)\n",
    "knn_2.fit(X_train_2, y_train_2)\n",
    "\n",
    "knn_5 = KNN(n_neighbors=5)\n",
    "knn_5.fit(X_train_2, y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(2, 2, figsize = (12,12))\n",
    "\n",
    "plot_decision_boundary(X_train_2.values, y_train_2.values, \n",
    "                               knn_2, \"KNN 2 - Train\", axes[0,0])\n",
    "\n",
    "plot_decision_boundary(X_train_2.values, y_train_2.values, \n",
    "                               knn_5, \"KNN 5 - Train\", axes[0,1])\n",
    "\n",
    "plot_decision_boundary(X_test_2.values, y_test_2.values, \n",
    "                               knn_2, \"KNN 2 - Test\", axes[1,0])\n",
    "\n",
    "plot_decision_boundary(X_test_2.values, y_test_2.values, \n",
    "                               knn_5, \"KNN 5 - Test\", axes[1,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Train KNN 2:\", knn_2.score(X_train_2, y_train_2))\n",
    "print(\"Test KNN 2:\", knn_2.score(X_test_2, y_test_2))\n",
    "print(\"Train KNN 5:\", knn_5.score(X_train_2, y_train_2))\n",
    "print(\"Test KNN 5:\", knn_5.score(X_test_2, y_test_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now move on to the Midterm 2 solutions from last year:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Diagnosing the Simian Flu 2016\n",
    "\n",
    "You are given the early data for an outbreak of a dangerous virus originating from a group of primates being kept in a Massechussetts biomedical research lab, this virus is dubbed the \"Simian Flu\".\n",
    "\n",
    "You have the medical records of $n$ number of patients in `'flu_train.csv`. There are two general types of patients in the data, flu patients and healthy (this is recorded in the column labeled `flu`, a 0 indicates the absences of the virus and a 1 indicates presence). Furthermore, scientists have found that there are two strains of the virus, each requiring a different type of treatment (this is recorded in the column labeled `flutype`, a 1 indicates the absences of the virus, a 2 indicates presence of strain 1 and a 3 indicates the presence of strain 2).\n",
    "\n",
    "**Your task:** build a model to predict if a given patient has the flu. Your goal is to catch as many flu patients as possible without misdiagnosing too many healthy patients.\n",
    "\n",
    "**The deliverable:** a function called `flu_predict` which satisfies:\n",
    "\n",
    "- input: `x_test`, a set of medical predictors for a group of patients\n",
    "- output: `y_pred`, a set of labels, one for each patient; 0 for healthy and 1 for infected with the flu virus\n",
    "\n",
    "The MA state government will use your model to diagnose sets of future patients (held by us). You can expect that there will be an increase in the number of flu patients in any groups of patients in the future.\n",
    "\n",
    "We provide you with some benchmarks for comparison.\n",
    "\n",
    "**Baseline Model:** \n",
    "- ~50% expected accuracy on healthy patients in observed data\n",
    "- ~50% expected accuracy on flu patients in observed data\n",
    "- ~50% expected accuracy on healthy patients in future data \n",
    "- ~50% expected accuracy on flu patients in future data\n",
    "- time to build: 5 min\n",
    "\n",
    "**Reasonable Model:** \n",
    "- ~69% expected accuracy on healthy patients in observed data\n",
    "- ~55% expected accuracy on flu patients, in observed data\n",
    "- ~69% expected accuracy on healthy patients in future data\n",
    "- ~60% expected accuracy on flu patients, in future data\n",
    "- time to build: 20 min\n",
    "\n",
    "**Grading:**\n",
    "Your grade will be based on:\n",
    "1. your model's ability to out-perform our benchmarks\n",
    "2. your ability to carefully and thoroughly follow the data science pipeline (see lecture slides for definition)\n",
    "3. the extend to which all choices are reasonable and defensible by methods you have learned in this class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solutions:**\n",
    "\n",
    "## Step 1: Read the data, clean and explore the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flu_train = pd.read_csv('https://raw.githubusercontent.com/cs109/a-2017/master/Midterms/2016%20Midterm%202/data/flu_train.csv')\n",
    "flu_test = pd.read_csv('https://raw.githubusercontent.com/cs109/a-2017/master/Midterms/2016%20Midterm%202/data/flu_test.csv')\n",
    "\n",
    "flu_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flu_train.flu.mean(),flu_test.flu.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So first, only about 6% of the patients have the flu **in our training data**, and in terms of the predictors, we have a mixture of categorical and numerical variables with a fair amount of both missing and semi-duplicate information. To train a model to predict whether new patients will have the flu, we'll have to convert the categorical variables to indicators and handle all of the missing values.\n",
    "\n",
    "Let's inspect the column types and how many missing values they have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def column_type(column, df):\n",
    "    return 'categ' if df[column].dtype == np.dtype('O') else 'numer'\n",
    "\n",
    "def nan_fraction(column, df):\n",
    "    return len(df[df[column].isnull()]) * 1. / len(df)\n",
    "\n",
    "for i, column in enumerate(flu_train.columns):\n",
    "    if column == 'flu' or column == 'flutype':\n",
    "        continue\n",
    "    elif i % 2 == 0:\n",
    "        print('')\n",
    "    else:\n",
    "        print('\\t', end='')\n",
    "    print('{:16} ({}), NaNs: {:.2f}/{:.2f} train/test'.format(\n",
    "            column, column_type(column, flu_train),\n",
    "            nan_fraction(column, flu_train),\n",
    "            nan_fraction(column, flu_test)), end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a large number of missing values in the data. Nearly all predictors have some degree of missingness. Some include potentially useful information, while others do not.  NaN in the `'pregnancy'` column is meaningful and informative, as patients with NaN's in the pregnancy column are males, where as NaN's in other predictors may appear randomly. \n",
    "\n",
    "As a first pass, we drop any columns with an extremely high fraction of NaNs (along with any other columns that explicitly don't make sense to include, like `ID`). We encode the remaining categorical columns, reserving a separate category for missing values. We do mean imputation on the remaining quantitative variables. \n",
    "\n",
    "However, there are many acceptable strategies here, as long as the appropriateness of the method in the context of the task and the data is discussed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_and_impute(df):\n",
    "    # don't modify the original frame\n",
    "    df = df.copy()\n",
    "    del df['ID']\n",
    "    del df['AgeMonths'] # almost a duplicate of Age but with more NaNs\n",
    "    \n",
    "    for column in df.columns:\n",
    "        if nan_fraction(column, flu_train) > 0.75:\n",
    "            print(\"Dropping {0}\".format(column))\n",
    "            del df[column]\n",
    "    \n",
    "    encode = preprocessing.LabelEncoder()\n",
    "\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == np.object:\n",
    "            df[column] = df[column].fillna('NaN') #Categorical blank values get set as separate category \n",
    "            df.loc[:, column] = encode.fit_transform(df[column])\n",
    "        elif nan_fraction(column, flu_train) > 0:\n",
    "            df[column+'_missing'] = df[column].isnull().astype(int)\n",
    "            df[column] = df[column].fillna(flu_train[column].mean()) \n",
    "            \n",
    "    df = df[[c for c in df.columns if c not in ['flu','flutype']] + ['flu','flutype']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Clean and encode\n",
    "\n",
    "train = clean_and_impute(flu_train)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = clean_and_impute(flu_test)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#What's up in each set\n",
    "\n",
    "x = train[[c for c in train.columns if c not in ['flu','flutype']]].values\n",
    "y = train[['flu']].values.reshape(len(x))\n",
    "\n",
    "x_test = test[[c for c in train.columns if c not in ['flu','flutype']]].values\n",
    "y_test = test[['flu']].values.reshape(len(x_test))\n",
    "\n",
    "print('x train shape:', x.shape)\n",
    "print('x test shape:', x_test.shape)\n",
    "print('train class 0: {}, train class 1: {}'.format(len(y[y==0]), len(y[y==1])))\n",
    "print('train class 0: {}, train class 1: {}'.format(len(y_test[y_test==0]), len(y_test[y_test==1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Model Choice\n",
    "\n",
    "The first task is to decide which, of the large number of classifiers we have learned during this semester, would best suit our task and our data.\n",
    "\n",
    "It would be possible to do brute force model comparison here - i.e. tune all models and compare which does best with respect to various benchmarks. However, it is also reasonable to do a first round of model comparison by running models (with out of the box parameter settings) on bootstrapped training data and eliminating models which performed very poorly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expected_score(model, x_train, y_train):\n",
    "    overall = 0\n",
    "    class_0 = 0\n",
    "    class_1 = 0\n",
    "    for i in range(100):\n",
    "        np.random.seed(i)\n",
    "        sample = np.random.choice(len(x_train), len(x_train))\n",
    "        x_sub_train = x_train[sample]\n",
    "        y_sub_train = y_train[sample]\n",
    "        \n",
    "        overall += model.score(x_sub_train, y_sub_train)\n",
    "        class_0 += model.score(x_sub_train[y_sub_train==0], y_sub_train[y_sub_train==0])\n",
    "        class_1 += model.score(x_sub_train[y_sub_train==1], y_sub_train[y_sub_train==1])\n",
    "\n",
    "    return pd.Series([overall / 100., \n",
    "                      class_0 / 100.,\n",
    "                      class_1 / 100.],\n",
    "                      index=['overall accuracy', 'accuracy on class 0', 'accuracy on class 1'])\n",
    "\n",
    "score = lambda model, x_train, y_train: pd.Series([model.score(x_train, y_train), \n",
    "                                                 model.score(x_train[y_train==0], y_train[y_train==0]),\n",
    "                                                 model.score(x_train[y_train==1], y_train[y_train==1])], \n",
    "                                                index=['overall accuracy', 'accuracy on class 0', 'accuracy on class 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#KNN\n",
    "knn = KNN(n_neighbors=2)\n",
    "knn.fit(x, y)\n",
    "\n",
    "knn_scores = score(knn, x, y)\n",
    "print('knn')\n",
    "\n",
    "#Unweighted logistic regression\n",
    "unweighted_logistic = LogisticRegression(C=1000)\n",
    "unweighted_logistic.fit(x, y)\n",
    "\n",
    "unweighted_log_scores = score(unweighted_logistic, x, y)\n",
    "print('unweighted log')\n",
    "\n",
    "\n",
    "#Weighted logistic regression\n",
    "weighted_logistic = LogisticRegression(C=1000, class_weight='balanced')\n",
    "weighted_logistic.fit(x, y)\n",
    "\n",
    "weighted_log_scores = score(weighted_logistic, x, y)\n",
    "print('weighted log')\n",
    "\n",
    "\n",
    "#LDA\n",
    "lda = LDA()\n",
    "lda.fit(x, y)\n",
    "\n",
    "lda_scores = score(lda, x, y)\n",
    "print('lda')\n",
    "\n",
    "#QDA\n",
    "qda = QDA()\n",
    "qda.fit(x, y)\n",
    "\n",
    "qda_scores = score(qda, x, y)\n",
    "print('qda')\n",
    "\n",
    "#Decision Tree\n",
    "tree = DecisionTree(max_depth=50, class_weight='balanced', criterion='entropy')\n",
    "tree.fit(x, y)\n",
    "\n",
    "tree_scores = score(tree, x, y)\n",
    "print('tree')\n",
    "\n",
    "\n",
    "#Random Forest\n",
    "rf = RandomForest(class_weight='balanced')\n",
    "rf.fit(x, y)\n",
    "\n",
    "rf_scores = score(rf, x, y)\n",
    "\n",
    "print('rf')\n",
    "\n",
    "#SVC\n",
    "svc = SVC(C=100, class_weight='balanced')\n",
    "svc.fit(x, y)\n",
    "\n",
    "svc_scores = score(svc, x, y)\n",
    "\n",
    "print('svc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Score Dataframe\n",
    "score_df = pd.DataFrame({'knn': knn_scores, \n",
    "                         'unweighted logistic': unweighted_log_scores,\n",
    "                         'weighted logistic': weighted_log_scores,\n",
    "                         'lda': lda_scores,\n",
    "                         'qda': qda_scores,\n",
    "                         'tree': tree_scores,\n",
    "                         'rf': rf_scores, \n",
    "                         'svc': svc_scores})\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we can rule out KNN, LDA and unweighted logistic. \n",
    "\n",
    "**What we do:** We are going to pick weighted logistic regression and just tune the regularization parameter to beat the test benchmarks. Can you figure out why we chose this course of action? Hint: Instead of looking at overall accuracy, look at accuracy on class 1. \n",
    "\n",
    "**What's probably good to do:** QDA, random forest, tree, SVC and weighted logistic are beating our train benchmarks as is. We will tune them to beat the test benchmarks by picking the model and parameter set with the highest CV accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Cs = 10.**np.arange(-3, 4, 1)\n",
    "scores = []\n",
    "for C in Cs:\n",
    "    print('C:', C)\n",
    "    weighted_log_scores = np.array([0., 0., 0.])\n",
    "    kf = KFold(len(x), n_folds=10, shuffle=True, random_state=10)\n",
    "    for train_index, test_index in kf:\n",
    "        x_validate_train, x_validate_test = x[train_index], x[test_index]\n",
    "        y_validate_train, y_validate_test = y[train_index], y[test_index]\n",
    "\n",
    "        weighted_logistic = LogisticRegression(C=C, class_weight='balanced')\n",
    "        weighted_logistic.fit(x_validate_train, y_validate_train)\n",
    "\n",
    "        weighted_log_scores += score(weighted_logistic, x_validate_test, y_validate_test).values\n",
    "\n",
    "    scores.append(weighted_log_scores / 10.)\n",
    "\n",
    "scores = pd.DataFrame(np.array(scores).T, columns=[str(C) for C in Cs], index=['overall accuracy', 'accuracy on class 0', 'accuracy on class 1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What C means here:** The `C` parameter in the `LogisticRegression` method is the inverse of the regularization parameter $\\lambda$. In other words, `C` $= \\frac{1}{\\lambda}$. As C gets larger, $\\lambda$ is getting smaller. Therefore, when we set `C` to be very very large in our earlier homework assignments to make sure there was **no** regularization, this was the same as making $\\lambda$ go to zero. When $\\lambda$ is zero, there is no regularization applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(scores.T['accuracy on class 1'].max())\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To beat the future benchmark, we'll select the parameter which yields the highest accuracy on class 1 (while still beating the benchmark on class 0).\n",
    "\n",
    "Now let's test our model on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Weighted logistic regression\n",
    "weighted_logistic = LogisticRegression(C=10, class_weight='balanced')\n",
    "weighted_logistic.fit(x, y)\n",
    "weighted_log_scores = score(weighted_logistic, x_test, y_test)\n",
    "weighted_log_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay, we beat all the benchmarks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Diagnosing Strains of the Simian Flu\n",
    "\n",
    "From a public health perspective, we want to balance the cost of vaccinations, early interventions and the cost of treating flu complications of unvaccinated people. \n",
    "\n",
    "There are two different strains of the flu: strain 1 has a cheaper early intervention as well as a cheaper treatment for flu complications, but patients with strain 1 has a higher rate of developing complications if treated with the wrong intervention. Strain 2 has a more expensive early intervention as well as a more costly treatment for flu complications, but patients with strain 2 has a lower rate of developing complications if treated with the wrong intervention. With no intervention, flu patients develop complications at the same rate regardless of the strain. \n",
    "\n",
    "**Your task:** build a model to predict if a given patient has the flu and identify the flu strain. The state government of MA will use your model to inform public health policies: we will vaccinate people you've identified as healthy and apply corresponding interventions to patients with different strains of the flu. We have provided you with a function to compute the total expected cost of this policy decision that takes into account the cost of the vaccine, the interventions and the cost of the treatments for flu complications resulting from misdiagnosing patients. Your goal is to make sure your model produces a public health policy with the lowest associated expected cost.\n",
    "\n",
    "**The deliverable:** a function called `flu_predict` which satisfies:\n",
    "\n",
    "- input: `x_test`, a set of medical predictors for a group of patients\n",
    "- output: `y_pred`, a set of labels, one for each patient; 0 for healthy, 1 for infected with strain 1, and 2 for infected with strain 2.\n",
    "\n",
    "The MA state government will use your model to diagnose sets of future patients (held by us). You can expect that there will be an increase in the number of flu patients in any groups of patients in the future.\n",
    "\n",
    "We provide you with some benchmarks for comparison.\n",
    "\n",
    "**Three Baseline Models:** \n",
    "- expected cost on observed data: \\$6,818,206.0, \\$7,035,735.0, \\$8,297,197.5\n",
    "- time to build: 1 min\n",
    "\n",
    "**Reasonable Model:** \n",
    "- expected cost on observed data: $6,300,000\n",
    "- time to build: 20 min\n",
    "\n",
    "**Grading:**\n",
    "Your grade will be based on:\n",
    "1. your model's ability to out-perform our benchmarks\n",
    "2. your ability to carefully and thoroughly follow the data science pipeline (see lecture slides for definition)\n",
    "3. the extend to which all choices are reasonable and defensible by methods you have learned in this class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------  cost\n",
    "# A function that computes the expected cost of the public healthy policy based on the \n",
    "# classifications generated by your model\n",
    "# Input: \n",
    "#      y_true (true class labels: 0, 1, 2)\n",
    "#      y_pred (predicted class labels: 0, 1, 2)\n",
    "# Returns: \n",
    "#      total_cost (expected total cost)\n",
    "\n",
    "def cost(y_true, y_pred):\n",
    "    cost_of_treatment_1 = 29500\n",
    "    cost_of_treatment_2 = 45000\n",
    "    cost_of_intervention_1 = 4150\n",
    "    cost_of_intervention_2 = 4250\n",
    "    cost_of_vaccine = 15\n",
    "    \n",
    "    prob_complications_untreated = 0.65\n",
    "    prob_complications_1 = 0.30\n",
    "    prob_complications_2 = 0.15\n",
    "    \n",
    "    trials = 1000    \n",
    "    \n",
    "    intervention_cost = cost_of_intervention_1 * len(y_pred[y_pred==1]) + cost_of_intervention_2 * len(y_pred[y_pred==2])\n",
    "\n",
    "    vaccine_cost = cost_of_vaccine * len(y_pred[y_pred==0])\n",
    "    \n",
    "    false_neg_1 = ((y_true == 1) & (y_pred == 2)).sum()\n",
    "    false_neg_2 = ((y_true == 2) & (y_pred == 1)).sum()\n",
    "    \n",
    "    untreated_1 = ((y_true == 1) & (y_pred == 0)).sum()    \n",
    "    untreated_2 = ((y_true == 2) & (y_pred == 0)).sum()\n",
    "    \n",
    "    false_neg_1_cost = np.random.binomial(1, prob_complications_1, (false_neg_1, trials)) * cost_of_treatment_1\n",
    "    false_neg_2_cost = np.random.binomial(1, prob_complications_2, (false_neg_2, trials)) * cost_of_treatment_2\n",
    "    untreated_1_cost = np.random.binomial(1, prob_complications_untreated, (untreated_1, trials)) * cost_of_treatment_1\n",
    "    untreated_2_cost = np.random.binomial(1, prob_complications_untreated, (untreated_2, trials)) * cost_of_treatment_2\n",
    "    \n",
    "    false_neg_1_cost = false_neg_1_cost.sum(axis=0)\n",
    "    expected_false_neg_1_cost = false_neg_1_cost.mean()\n",
    "    \n",
    "    false_neg_2_cost = false_neg_2_cost.sum(axis=0)\n",
    "    expected_false_neg_2_cost = false_neg_2_cost.mean()\n",
    "    \n",
    "    untreated_1_cost = untreated_1_cost.sum(axis=0)\n",
    "    expected_untreated_1_cost = untreated_1_cost.mean()\n",
    "    \n",
    "    untreated_2_cost = untreated_2_cost.sum(axis=0)\n",
    "    expected_untreated_2_cost = untreated_2_cost.mean()\n",
    "    \n",
    "    total_cost = vaccine_cost + intervention_cost + expected_false_neg_1_cost + expected_false_neg_2_cost + expected_untreated_1_cost + expected_untreated_2_cost\n",
    "    \n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're just going to take the weighted logistic model, again, and tune the regularization parameter to both beat the benchmark on the observed data and minimize expected cost on unseen data (i.e. to prevent ***overfitting***). Instead of using 'balanced' class weights, we're using a custom weighting scheme for the three classes (this parameter should really be tuned!).\n",
    "\n",
    "It would probally also be go through the whole \"choosing a model, tuning these models\"-process again, this time to minimize cost.\n",
    "\n",
    "**Note:** Be aware that the cost is now sensitive to sample size! The smaller the pool of patients the less the cost. If you are evaluating cost on a held-out test set then you can artificially make the cost very small. The benchmarks we give are for the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = train.values[:, :-2]\n",
    "y = train.values[:, -1]\n",
    "y = y - 1\n",
    "\n",
    "x_test = test.values[:, :-2]\n",
    "y_test = test.values[:, -1]\n",
    "\n",
    "y_test = y_test - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = lambda model, x_test, y_test: pd.Series([model.score(x_test, y_test), \n",
    "                                                 model.score(x_test[y_test==0], y_test[y_test==0]),\n",
    "                                                 model.score(x_test[y_test==1], y_test[y_test==1]), \n",
    "                                                 model.score(x_test[y_test==2], y_test[y_test==2]), \n",
    "                                                 cost(y_test, model.predict(x_test))],\n",
    "                                                index=['overall accuracy', 'accuracy on class 0', 'accuracy on class 1', 'accuracy on class 2', 'total cost'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let’s make some benchmarks to compare the cost of our model against...\n",
    "\n",
    "First, let's cost a perfect model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = [('Perfect', cost(y, y), cost(y_test, y_test))] \n",
    "\n",
    "print('minimimum cost on train:', cost(y, y))\n",
    "print('minimimum cost on test:', cost(y_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we presume that all patients are healthy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l.append(('All Healthy', cost(y, np.array([0] * len(y))), cost(y_test, np.array([0] * len(y_test)))))\n",
    "\n",
    "print('simple model cost on train (all 0):', cost(y, np.array([0] * len(y))))\n",
    "print('simple model cost on test (all 0):', cost(y_test, np.array([0] * len(y_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All have strain 1 ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l.append(('All 1', cost(y, np.array([1] * len(y))),cost(y_test, np.array([1] * len(y_test)))))\n",
    "\n",
    "print('simple model cost on train (all 1):', cost(y, np.array([1] * len(y))))\n",
    "print('simple model cost on test: (all 1):', cost(y_test, np.array([1] * len(y_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All have strain 2 ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l.append(('All 2', cost(y, np.array([2] * len(y))),cost(y_test, np.array([2] * len(y_test)))))\n",
    "\n",
    "print('simple model cost on train (all 2):', cost(y, np.array([2] * len(y))))\n",
    "print('simple model cost on test (all 2):', cost(y_test, np.array([2] * len(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(l,columns=[\"Model\",\"Cost on Train (Mil.)\",\"Cost on Test (Mil.)\"]).set_index('Model')/10**6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s cost out the models we trained above…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = {'knn': knn, \n",
    "         'unweighted logistic': unweighted_logistic,\n",
    "         'weighted logistic': weighted_logistic,\n",
    "        'lda': lda,\n",
    "         'qda': qda,\n",
    "         'tree': tree,\n",
    "          'rf': rf, \n",
    "         'svc': svc}\n",
    "\n",
    "for k in models:\n",
    "    print(\"\\n\\n**************\")\n",
    "    print(k, \"Train\")\n",
    "    mod = models[k]\n",
    "    mod.fit(x, y)\n",
    "    scores = score(mod, x, y)\n",
    "    print(scores.round(decimals=3))\n",
    "    train_cost = scores['total cost']\n",
    "    scores = score(mod, x_test, y_test)\n",
    "    print(k, \"Train\")\n",
    "    print(scores.round(decimals=3))\n",
    "    test_cost = scores['total cost']\n",
    "    l.append((k,train_cost,test_cost))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(l,columns=[\"Model\",\"Cost on Train (Mil.)\",\"Cost on Test (Mil.)\"]).set_index('Model')/10**6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Logistic Reg...\n",
    "\n",
    "Now let’s see if we can beat our benchmarks…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Cs = 10.**np.arange(-3, 4, 1)\n",
    "scores = []\n",
    "for C in Cs:\n",
    "    print('C:', C)\n",
    "    weighted_log_scores = np.array([0., 0., 0., 0., 0.])\n",
    "    kf = KFold(len(x), n_folds=10, shuffle=True, random_state=10)\n",
    "    for train_index, test_index in kf:\n",
    "        x_validate_train, x_validate_test = x[train_index], x[test_index]\n",
    "        y_validate_train, y_validate_test = y[train_index], y[test_index]\n",
    "\n",
    "        weighted_logistic = LogisticRegression(C=C, class_weight={0:0.7, 1:10, 2:10})\n",
    "        weighted_logistic.fit(x_validate_train, y_validate_train)\n",
    "\n",
    "        weighted_log_scores += score(weighted_logistic, x_validate_test, y_validate_test).values\n",
    "\n",
    "    scores.append(weighted_log_scores / 10.)\n",
    "\n",
    "scores = pd.DataFrame(np.array(scores).T, columns=[str(C) for C in Cs], index=['overall accuracy', 'accuracy on class 0', 'accuracy on class 1', 'accuracy on class 2', 'total cost'])\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Weighted logistic regression on training set \n",
    "weighted_logistic = LogisticRegression(C=10, class_weight={0:0.7, 1:10, 2:10})\n",
    "weighted_logistic.fit(x, y)\n",
    "weighted_log_scores = score(weighted_logistic, x, y)\n",
    "weighted_log_scores.round(decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Weighted logistic regression on test set\n",
    "weighted_log_scores = score(weighted_logistic, x_test, y_test)\n",
    "weighted_log_scores.round(decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l.append(('Tuned Weighted logistic regression', score(weighted_logistic, x, y)['total cost'],score(weighted_logistic, x_test, y_test)['total cost']))\n",
    "pd.DataFrame(l,columns=[\"Model\",\"Cost on Train (Mil.)\",\"Cost on Test (Mil.)\"]).set_index('Model')/10**6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! We beat the benchmarks on the observed data and did pretty well on test data! Let's see if we can do even better..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
